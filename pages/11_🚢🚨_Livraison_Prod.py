import streamlit as st
import pandas as pd
import requests
import time
import yaml
from sqlalchemy import text
from utils.db import get_engine, get_engine_prod, get_engine_prod_writing

# Configuration de la page
st.set_page_config(layout="wide")
st.title("ğŸš¢ğŸš¨ Livraison des indicateurs en PRODUCTION")

st.markdown("""
Cette page compare les donnÃ©es **staging** (table `indicateurs_valeurs_olap`) 
avec les donnÃ©es en **production** (table `indicateur_valeur`) pour identifier :
- ğŸ†• Les nouveaux indicateurs Ã  importer
- ğŸ“… Les nouvelles annÃ©es pour des indicateurs existants
- ğŸ”„ Les donnÃ©es Ã  mettre Ã  jour
""")

st.error("âš ï¸ **ATTENTION : Cette page effectue des modifications en PRODUCTION !**")

st.markdown("---")

# ==========================
# FONCTIONS
# ==========================

def load_staged_data():
    """Charge les donnÃ©es de la table indicateurs_valeurs_olap avec l'identifiant_referentiel et api_nom_cube."""
    engine = get_engine()
    
    try:
        query = text("""
            SELECT 
                collectivite_id,
                indicateur_id,
                metadonnee_id,
                date_valeur,
                resultat,
                api_nom_cube,
                identifiant_referentiel
            FROM indicateurs_valeurs_olap
        """)
        
        with engine.connect() as conn:
            df = pd.read_sql_query(query, conn)
        
        # Convertir explicitement date_valeur en datetime
        df['date_valeur'] = pd.to_datetime(df['date_valeur'])
        
        return df
    except Exception as e:
        st.error(f"âŒ Erreur lors du chargement des donnÃ©es staging : {str(e)}")
        return pd.DataFrame()


def transform_staged_for_prod(df_staged):
    """Transforme les donnÃ©es staged pour les adapter Ã  la production.
    
    Effectue deux transformations principales :
    1. Mapping des indicateur_id via identifiant_referentiel
    2. Mapping des metadonnee_id (avec insertion en prod si nÃ©cessaire)
    
    Args:
        df_staged: DataFrame avec les donnÃ©es staging (IDs preprod)
    
    Returns:
        DataFrame avec les IDs prod
    """
    if df_staged.empty:
        return df_staged
    
    # VÃ©rifier que les engines sont bien configurÃ©s
    try:
        engine_prod = get_engine_prod()
        engine_prod_writing = get_engine_prod_writing()
    except Exception as e:
        st.error(f"âŒ Erreur de configuration des engines de base de donnÃ©es : {str(e)}")
        st.error("VÃ©rifiez que les secrets 'database_prod' et 'database_prod_writing' sont bien configurÃ©s dans .streamlit/secrets.toml")
        return pd.DataFrame()
    
    df_result = df_staged.copy()
    
    # ============================================
    # 1. MAPPING DES INDICATEUR_ID
    # ============================================
    
    st.info("ğŸ”„ Mapping des indicateur_id (preprod â†’ prod)...")
    
    try:        
        # Charger le mapping depuis production
        with engine_prod.connect() as conn:
            mapping_prod = pd.read_sql_query(
                text("SELECT id as indicateur_id_prod, identifiant_referentiel FROM indicateur_definition WHERE collectivite_id IS NULL"),
                conn
            )

        mapping_dict = dict(zip(mapping_prod["identifiant_referentiel"], mapping_prod["indicateur_id_prod"]))
        
        # Appliquer le mapping
        df_result['indicateur_id'] = df_result['identifiant_referentiel'].map(mapping_dict)
        
        st.success(f"âœ… Mapping indicateur_id effectuÃ© : {len(mapping_dict)} indicateurs")
        
    except Exception as e:
        st.error(f"âŒ Erreur lors du mapping des indicateur_id : {str(e)}")
        return pd.DataFrame()
    
    # ============================================
    # 2. CHARGEMENT DU YAML
    # ============================================
    
    st.info("ğŸ”„ Chargement du fichier de configuration YAML...")
    
    try:
        # Charger le fichier YAML
        with open('utils/config.yaml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        indicateurs_yaml = config['indicateurs']
        st.success(f"âœ… {len(indicateurs_yaml)} indicateurs chargÃ©s depuis le YAML")
        
        # CrÃ©er un dictionnaire pour accÃ¨s rapide par api_nom_cube
        yaml_by_cube = {indic['api_nom_cube']: indic for indic in indicateurs_yaml}
        
        st.info(f"ğŸ“‹ {len(yaml_by_cube)} cubes trouvÃ©s dans le YAML")
        
    except Exception as e:
        st.error(f"âŒ Erreur lors du chargement du YAML : {str(e)}")
        import traceback
        st.error(traceback.format_exc())
        return pd.DataFrame()
    
    # ============================================
    # 3. INSERTION DES SOURCES ET METADONNEES EN PROD
    # ============================================
    
    st.info("ğŸ”„ Insertion/vÃ©rification des sources et mÃ©tadonnÃ©es en prod...")
    
    try:
        # RÃ©cupÃ©rer les api_nom_cube uniques de nos donnÃ©es
        cubes = df_result['api_nom_cube'].unique().tolist()
        
        # Dictionnaire pour stocker le mapping api_nom_cube -> metadonnee_id_prod
        dict_mapping_meta = {}
        nb_sources_inserees = 0
        nb_sources_existantes = 0
        nb_meta_insertions = 0
        nb_meta_updates = 0
        
        sources_traitees = set()  # Pour Ã©viter de traiter la mÃªme source plusieurs fois
        
        for cube in cubes:
            # RÃ©cupÃ©rer les infos depuis le YAML
            if cube not in yaml_by_cube:
                st.warning(f"âš ï¸ Cube '{cube}' non trouvÃ© dans le YAML, ignorÃ©")
                continue
            
            indic_yaml = yaml_by_cube[cube]
            source_info = indic_yaml['source']
            metadata_info = indic_yaml['metadata']
            
            # ====================================
            # A. INSERTION/VERIFICATION DE LA SOURCE
            # ====================================
            source_id = source_info['id']
            
            if source_id not in sources_traitees:
                sources_traitees.add(source_id)
                
                try:
                    with engine_prod_writing.begin() as conn:
                        # VÃ©rifier si la source existe dÃ©jÃ 
                        check_query = text("""
                            SELECT 1 FROM indicateur_source WHERE id = :id
                        """)
                        result = conn.execute(check_query, {"id": source_id}).first()
                        
                        if result is None:
                            # Insertion si la source n'existe pas encore
                            insert_query = text("""
                                INSERT INTO indicateur_source (id, libelle, ordre_affichage)
                                VALUES (:id, :libelle, :ordre_affichage)
                            """)
                            conn.execute(insert_query, {
                                "id": source_id,
                                "libelle": source_info["libelle"],
                                "ordre_affichage": source_info["ordre_affichage"]
                            })
                            nb_sources_inserees += 1
                        else:
                            nb_sources_existantes += 1
                except Exception as conn_error:
                    st.error(f"âŒ Erreur de connexion Ã  la base prod lors de l'insertion de la source '{source_id}' : {str(conn_error)}")
                    st.error("VÃ©rifiez que le secret 'database_prod_writing' est correctement configurÃ© dans .streamlit/secrets.toml")
                    raise
            
            # ====================================
            # B. INSERTION/UPDATE DE LA METADONNEE
            # ====================================
            
            # ClÃ© d'unicitÃ©
            cle = {
                "source_id": metadata_info["source_id"],
                "nom_donnees": metadata_info["nom_donnees"]
            }
            
            # Colonnes Ã  mettre Ã  jour si ligne dÃ©jÃ  existante
            update_fields = {
                "date_version": metadata_info["date_version"],
                "methodologie": metadata_info["methodologie"],
                "limites": metadata_info["limites"]
            }
            
            # Utiliser engine_prod pour la lecture
            with engine_prod.connect() as conn_read:
                # 1. Recherche d'une ligne existante selon la clÃ© logique
                result = conn_read.execute(text("""
                    SELECT id FROM indicateur_source_metadonnee
                    WHERE source_id = :source_id
                    AND nom_donnees = :nom_donnees
                    LIMIT 1;
                """), cle).first()
            
            # Utiliser engine_prod_writing pour l'Ã©criture
            with engine_prod_writing.begin() as conn_write:
                if result is None:
                    # 2. Si aucune ligne existante â†’ INSERT
                    meta_dict = {
                        "source_id": metadata_info["source_id"],
                        "date_version": metadata_info["date_version"],
                        "nom_donnees": metadata_info["nom_donnees"],
                        "diffuseur": metadata_info["diffuseur"],
                        "producteur": metadata_info["producteur"],
                        "methodologie": metadata_info["methodologie"],
                        "limites": metadata_info["limites"]
                    }
                    
                    result = conn_write.execute(text("""
                        INSERT INTO indicateur_source_metadonnee (
                            source_id, date_version, nom_donnees, diffuseur,
                            producteur, methodologie, limites
                        ) VALUES (
                            :source_id, :date_version, :nom_donnees, :diffuseur,
                            :producteur, :methodologie, :limites
                        ) RETURNING id;
                    """), meta_dict).first()
                    
                    id_prod = result[0]
                    dict_mapping_meta[cube] = id_prod
                    nb_meta_insertions += 1
                    
                else:
                    # 3. Si ligne existante â†’ UPDATE
                    id_prod = result[0]
                    update_fields["id"] = id_prod
                    
                    conn_write.execute(text("""
                        UPDATE indicateur_source_metadonnee
                        SET date_version = :date_version,
                            methodologie = :methodologie,
                            limites = :limites
                        WHERE id = :id;
                    """), update_fields)
                    
                    dict_mapping_meta[cube] = id_prod
                    nb_meta_updates += 1
        
        # Appliquer le mapping
        df_result['metadonnee_id'] = df_result['api_nom_cube'].map(dict_mapping_meta)
        
        st.success(f"âœ… Sources : {nb_sources_inserees} insertion(s), {nb_sources_existantes} existante(s)")
        st.success(f"âœ… MÃ©tadonnÃ©es : {nb_meta_insertions} insertion(s), {nb_meta_updates} mise(s) Ã  jour")
        
    except Exception as e:
        st.error(f"âŒ Erreur lors de l'insertion des sources/mÃ©tadonnÃ©es : {str(e)}")
        import traceback
        st.error(traceback.format_exc())
        return pd.DataFrame()
    
    # VÃ©rifier qu'il n'y a pas de valeurs nulles aprÃ¨s transformation
    if df_result['indicateur_id'].isna().any() or df_result['metadonnee_id'].isna().any():
        nb_nulls = df_result[['indicateur_id', 'metadonnee_id']].isna().sum()
        st.warning(f"âš ï¸ Valeurs nulles dÃ©tectÃ©es aprÃ¨s transformation : {nb_nulls.to_dict()}")
        st.warning("Suppression des lignes avec des valeurs nulles...")
        df_result = df_result.dropna(subset=['indicateur_id', 'metadonnee_id'])
    
    # Convertir les IDs en int (peuvent Ãªtre en float aprÃ¨s le mapping)
    df_result['indicateur_id'] = df_result['indicateur_id'].astype(int)
    df_result['metadonnee_id'] = df_result['metadonnee_id'].astype(int)
    
    # Supprimer les colonnes qui ne servent plus
    df_result = df_result.drop(columns=['identifiant_referentiel', 'api_nom_cube'])
    
    st.success(f"âœ… Transformation terminÃ©e : {len(df_result):,} lignes prÃªtes pour la prod")
    
    return df_result


def load_indicateurs_titres():
    """Charge le mapping id -> titre des indicateurs depuis la production."""
    engine_prod = get_engine_prod()
    
    try:
        query = text("""
            SELECT DISTINCT id, titre
            FROM indicateur_definition
            WHERE collectivite_id IS NULL
        """)
        
        with engine_prod.connect() as conn:
            df = pd.read_sql_query(query, conn)
        
        # CrÃ©er un dictionnaire pour le mapping
        mapping = dict(zip(df['id'], df['titre']))
        
        return mapping
    except Exception as e:
        st.error(f"âŒ Erreur lors du chargement des titres d'indicateurs : {str(e)}")
        return {}


def load_prod_data(df_staged):
    """Charge les donnÃ©es de la table indicateur_valeur en production.
    
    Ne charge QUE les donnÃ©es correspondant aux clÃ©s primaires prÃ©sentes dans le staging
    pour optimiser les performances sur les grosses tables.
    
    Args:
        df_staged: DataFrame staging pour extraire les clÃ©s primaires Ã  charger
    """
    engine_prod = get_engine_prod()
    
    if df_staged.empty:
        return pd.DataFrame()
    
    try:
        # Extraire les valeurs uniques pour chaque colonne de la clÃ© primaire
        indicateurs_ids = df_staged['indicateur_id'].unique().tolist()
        collectivite_ids = df_staged['collectivite_id'].unique().tolist()
        metadonnee_ids = df_staged['metadonnee_id'].unique().tolist()
        
        st.info(f"ğŸ” Filtrage production : {len(indicateurs_ids)} indicateurs, {len(collectivite_ids)} collectivitÃ©s, {len(metadonnee_ids)} mÃ©tadonnÃ©es")
        
        # RequÃªte avec filtres sur les clÃ©s primaires - ne charger que les colonnes nÃ©cessaires
        query = text("""
            SELECT 
                collectivite_id,
                indicateur_id,
                metadonnee_id,
                date_valeur,
                resultat
            FROM indicateur_valeur
            WHERE indicateur_id = ANY(:indicateurs_ids)
              AND collectivite_id = ANY(:collectivite_ids)
              AND metadonnee_id = ANY(:metadonnee_ids)
        """)
        
        with engine_prod.connect() as conn:
            df = pd.read_sql_query(
                query, 
                conn, 
                params={
                    'indicateurs_ids': indicateurs_ids,
                    'collectivite_ids': collectivite_ids,
                    'metadonnee_ids': metadonnee_ids
                }
            )
        
        # Convertir explicitement date_valeur en datetime
        df['date_valeur'] = pd.to_datetime(df['date_valeur'])
        
        st.success(f"âœ… {len(df):,} lignes chargÃ©es depuis la production")
        
        return df
    except Exception as e:
        st.error(f"âŒ Erreur lors du chargement des donnÃ©es production : {str(e)}")
        return pd.DataFrame()


def get_valid_collectivite_ids_prod():
    """RÃ©cupÃ¨re la liste des collectivite_id valides depuis la table collectivite en production.
    
    Returns:
        set: Ensemble des collectivite_id valides
    """
    engine_prod = get_engine_prod()
    
    try:
        query = text("SELECT id FROM collectivite")
        
        with engine_prod.connect() as conn:
            df = pd.read_sql_query(query, conn)
        
        valid_ids = set(df['id'].tolist())
        st.info(f"âœ… {len(valid_ids)} collectivitÃ©s valides trouvÃ©es en production")
        
        return valid_ids
    except Exception as e:
        st.error(f"âŒ Erreur lors de la rÃ©cupÃ©ration des collectivitÃ©s production : {str(e)}")
        return set()


def livrer_en_prod(comparison, df_staged, progress_container=None):
    """Fait l'upsert des donnÃ©es staging vers la production via API.
    
    N'envoie que les donnÃ©es qui ont vraiment changÃ© :
    - Nouveaux indicateurs
    - Nouvelles annÃ©es
    - DonnÃ©es avec rÃ©sultats diffÃ©rents
    
    Filtre automatiquement les donnÃ©es pour ne garder que les collectivite_id
    qui existent en production (Ã©vite les violations de clÃ© Ã©trangÃ¨re).
    
    Args:
        comparison: RÃ©sultats de la comparaison staging vs production
        df_staged: DataFrame contenant toutes les donnÃ©es staging
        progress_container: Container Streamlit pour afficher la progression
    
    Returns:
        dict: Statistiques de la livraison
    """
    # Construire le DataFrame des donnÃ©es Ã  envoyer
    dfs_to_send = []
    
    # 1. Nouveaux indicateurs
    if len(comparison['nouveaux_indicateurs']) > 0:
        dfs_to_send.append(comparison['nouveaux_indicateurs'])
    
    # 2. Nouvelles annÃ©es
    if len(comparison['nouvelles_annees']) > 0:
        dfs_to_send.append(comparison['nouvelles_annees'])
    
    # 3. DonnÃ©es Ã  updater (extraire depuis les dataframes par indicateur)
    for indic_id, stats in comparison['donnees_a_updater'].items():
        if 'dataframe' in stats:
            df_update = stats['dataframe'].copy()
            # SÃ©lectionner les colonnes nÃ©cessaires et renommer
            df_update = df_update[['collectivite_id', 'indicateur_id', 'metadonnee_id', 'date_valeur', 'resultat_staged']]
            df_update = df_update.rename(columns={'resultat_staged': 'resultat'})
            dfs_to_send.append(df_update)
    
    # ConcatÃ©ner toutes les donnÃ©es Ã  envoyer
    if not dfs_to_send:
        return {'nb_total': 0, 'success': True, 'message': 'Aucune donnÃ©e Ã  livrer (tout est dÃ©jÃ  Ã  jour)'}
    
    df_to_send = pd.concat(dfs_to_send, ignore_index=True)
    
    # Filtrer pour ne garder que les collectivite_id qui existent en production
    nb_lignes_avant = len(df_to_send)
    nb_lignes_filtrees = 0
    valid_collectivite_ids = get_valid_collectivite_ids_prod()
    
    if valid_collectivite_ids:
        df_to_send = df_to_send[df_to_send['collectivite_id'].isin(valid_collectivite_ids)]
        nb_lignes_apres = len(df_to_send)
        nb_lignes_filtrees = nb_lignes_avant - nb_lignes_apres
        
        if nb_lignes_filtrees > 0:
            st.warning(f"âš ï¸ {nb_lignes_filtrees} ligne(s) filtrÃ©e(s) (collectivitÃ©s inexistantes en production)")
        
        if nb_lignes_apres == 0:
            return {
                'nb_total': 0, 
                'nb_filtered': nb_lignes_filtrees,
                'success': True, 
                'message': f'Aucune donnÃ©e Ã  livrer aprÃ¨s filtrage des collectivitÃ©s ({nb_lignes_filtrees} lignes filtrÃ©es)'
            }
    
    try:
        # RÃ©cupÃ©rer les credentials depuis les secrets
        try:
            api_url = st.secrets.get("api_prod_url", "https://api.territoiresentransitions.fr/indicateurs/valeurs")
            api_token = st.secrets.get("api_prod_token", "")
        except:
            return {
                'nb_total': 0,
                'success': False,
                'message': 'Configuration API manquante dans secrets.toml'
            }
        
        headers = {
            "Authorization": f"Bearer {api_token}",
            "Content-Type": "application/json"
        }
        
        # 1. PrÃ©parer la liste de dicts Ã  partir du DataFrame
        df_to_insert = df_to_send.copy()
        df_to_insert['date_valeur'] = pd.to_datetime(df_to_insert['date_valeur'])
        
        valeurs_payload = df_to_insert.apply(lambda row: {
            "collectiviteId": int(row["collectivite_id"]),
            "indicateurId": int(row["indicateur_id"]),
            "dateValeur": row["date_valeur"].isoformat(),
            "metadonneeId": int(row["metadonnee_id"]),
            "resultat": float(row["resultat"]) if pd.notnull(row["resultat"]) else None,
        }, axis=1).tolist()

        print(f"{valeurs_payload}")
        
        # 2. ParamÃ©trage du batch
        batch_size = 500
        total_rows = len(valeurs_payload)
        total_inserted = 0
        failed_batches = 0
        
        max_batches_per_minute = 90
        pause_seconds = 60
        
        # Calculer le nombre total de batches
        total_batches = (total_rows + batch_size - 1) // batch_size
        
        if progress_container:
            progress_bar = progress_container.progress(0)
            status_text = progress_container.empty()
        
        # 3. Envoi des batchs
        for batch_start in range(0, total_rows, batch_size):
            batch_num = batch_start // batch_size + 1
            
            # Pause si on atteint un multiple de 90 (sauf au tout dÃ©but)
            if batch_num > 1 and (batch_num - 1) % max_batches_per_minute == 0:
                if progress_container:
                    status_text.info(f"â¸ï¸ Pause de {pause_seconds}s pour respecter la limite de 90 requÃªtes/minute...")
                time.sleep(pause_seconds)
            
            batch = valeurs_payload[batch_start : batch_start + batch_size]
            payload = {"valeurs": batch}
            
            if progress_container:
                status_text.text(f"ğŸ“¤ Envoi du batch {batch_num}/{total_batches} ({len(batch)} lignes)...")
            
            response = requests.post(api_url, headers=headers, json=payload)
            
            if response.status_code == 201:
                # Si l'API renvoie un objet { "valeurs": [...] }
                try:
                    inserted = len(response.json().get("valeurs", []))
                except ValueError:
                    inserted = len(batch)
                total_inserted += inserted
                
                if progress_container:
                    status_text.success(f"âœ… Batch {batch_num}/{total_batches} OK ({inserted} lignes)")
            else:
                failed_batches += 1
                
                # Logger l'erreur dans la console
                print(f"\n{'='*80}")
                print(f"âŒ ERREUR - Batch {batch_num}/{total_batches} Ã©chouÃ© (HTTP {response.status_code})")
                print(f"{'='*80}")
                
                try:
                    error_json = response.json()
                    print("RÃ©ponse API (JSON):")
                    import json
                    print(json.dumps(error_json, indent=2, ensure_ascii=False))
                except:
                    print("RÃ©ponse serveur (texte):")
                    print(response.text)
                
                print(f"{'='*80}\n")
                
                if progress_container:
                    status_text.error(f"âŒ Batch {batch_num}/{total_batches} Ã©chouÃ© (HTTP {response.status_code}) - voir console pour dÃ©tails")
                
                # On continue mÃªme en cas d'erreur sur un batch
            
            # Mettre Ã  jour la barre de progression
            if progress_container:
                progress_bar.progress(batch_num / total_batches)
        
        if progress_container:
            status_text.empty()
            progress_bar.empty()
        
        success = failed_batches == 0
        message = 'Livraison rÃ©ussie' if success else f'Livraison partielle ({failed_batches} batch(s) Ã©chouÃ©(s))'
        
        return {
            'nb_total': total_rows,
            'nb_inserted': total_inserted,
            'nb_batches': total_batches,
            'nb_filtered': nb_lignes_filtrees,
            'failed_batches': failed_batches,
            'success': success,
            'message': message
        }
        
    except Exception as e:
        return {
            'nb_total': 0,
            'nb_inserted': 0,
            'nb_filtered': nb_lignes_filtrees if 'nb_lignes_filtrees' in locals() else 0,
            'success': False,
            'message': f'Erreur lors de la livraison : {str(e)}'
        }


def compare_data(df_staged, df_prod):
    """Compare les donnÃ©es staging et production avec une approche par merge.
    
    Returns:
        dict: {
            'nouveaux_indicateurs': DataFrame,
            'nouvelles_annees': DataFrame,
            'donnees_a_updater': dict with DataFrames by indicateur_id
        }
    """
    # Colonnes de la clÃ© primaire
    pk_cols = ['collectivite_id', 'indicateur_id', 'metadonnee_id', 'date_valeur']
    
    df_staged = df_staged.copy()
    df_prod = df_prod.copy()
    
    # 1. NOUVEAUX INDICATEURS
    indicateurs_staged = set(df_staged['indicateur_id'].unique())
    indicateurs_prod = set(df_prod['indicateur_id'].unique())
    nouveaux_indicateurs_ids = indicateurs_staged - indicateurs_prod
    
    df_nouveaux_indicateurs = df_staged[
        df_staged['indicateur_id'].isin(nouveaux_indicateurs_ids)
    ].copy()
    
    # 2. INDICATEURS EXISTANTS
    indicateurs_existants = indicateurs_staged & indicateurs_prod
    
    # 3. NOUVELLES ANNÃ‰ES ET DONNÃ‰ES Ã€ UPDATER - par indicateur
    donnees_a_updater = {}
    all_nouvelles_annees = []
    
    for indic_id in indicateurs_existants:
        # Filtrer par indicateur
        df_staged_indic = df_staged[df_staged['indicateur_id'] == indic_id].copy()
        df_prod_indic = df_prod[df_prod['indicateur_id'] == indic_id].copy()
        
        # Merge sur les clÃ©s primaires
        df_merge = df_staged_indic.merge(
            df_prod_indic,
            on=pk_cols,
            how='outer',
            suffixes=('_staged', '_prod'),
            indicator=True
        )
        
        # Nouvelles annÃ©es : prÃ©sent dans staging mais pas dans production
        df_nouvelles = df_merge[df_merge['_merge'] == 'left_only'].copy()
        if len(df_nouvelles) > 0:
            # Garder les colonnes staging (sans suffixe)
            cols_to_keep = pk_cols + [col for col in df_nouvelles.columns if col.endswith('_staged')]
            df_nouvelles = df_nouvelles[cols_to_keep].copy()
            # Renommer en enlevant le suffixe
            df_nouvelles.columns = [col.replace('_staged', '') for col in df_nouvelles.columns]
            all_nouvelles_annees.append(df_nouvelles)
        
        # DonnÃ©es Ã  updater : prÃ©sent dans les deux avec rÃ©sultats diffÃ©rents
        df_both = df_merge[df_merge['_merge'] == 'both'].copy()
        
        if len(df_both) > 0:
            # Comparer les rÃ©sultats
            df_both['resultat_diff'] = df_both['resultat_staged'] != df_both['resultat_prod']
            df_diff = df_both[df_both['resultat_diff']].copy()
            
            if len(df_diff) > 0:
                # Calculer l'Ã©cart en %
                df_diff['ecart_abs'] = df_diff['resultat_staged'] - df_diff['resultat_prod']
                
                # Ã‰cart en % (gÃ©rer division par zÃ©ro)
                df_diff['ecart_pct'] = 0.0
                mask_non_zero = df_diff['resultat_prod'] != 0
                df_diff.loc[mask_non_zero, 'ecart_pct'] = (
                    abs(df_diff.loc[mask_non_zero, 'ecart_abs'] / df_diff.loc[mask_non_zero, 'resultat_prod']) * 100
                ).round(0)
                
                # Pour les valeurs oÃ¹ production = 0 mais staged != 0
                mask_div_zero = (df_diff['resultat_prod'] == 0) & (df_diff['resultat_staged'] != 0)
                df_diff.loc[mask_div_zero, 'ecart_pct'] = float('inf')
                
                # SÃ©lectionner les colonnes pertinentes
                cols_result = pk_cols + ['resultat_prod', 'resultat_staged', 'ecart_abs', 'ecart_pct']
                df_result = df_diff[cols_result].copy()
                
                # Calculer les statistiques
                df_result_finis = df_result[df_result['ecart_pct'] != float('inf')]
                
                if len(df_result_finis) > 0:
                    ecart_moyen = df_result_finis['ecart_pct'].mean()
                    idx_max = df_result_finis['ecart_pct'].idxmax()
                    max_row = df_result_finis.loc[idx_max]
                    
                    donnees_a_updater[indic_id] = {
                        'nb_lignes': len(df_result),
                        'ecart_moyen_pct': ecart_moyen,
                        'ecart_max_pct': max_row['ecart_pct'],
                        'collectivite_id_max': max_row['collectivite_id'],
                        'date_valeur_max': max_row['date_valeur'],
                        'resultat_prod_max': max_row['resultat_prod'],
                        'resultat_staged_max': max_row['resultat_staged'],
                        'dataframe': df_result.sort_values('ecart_pct', ascending=False)
                    }
                else:
                    # Tous les Ã©carts sont des divisions par zÃ©ro
                    donnees_a_updater[indic_id] = {
                        'nb_lignes': len(df_result),
                        'ecart_moyen_pct': None,
                        'message': 'Tous les Ã©carts sont de division par zÃ©ro',
                        'dataframe': df_result
                    }
    
    # ConcatÃ©ner toutes les nouvelles annÃ©es
    if all_nouvelles_annees:
        df_nouvelles_annees = pd.concat(all_nouvelles_annees, ignore_index=True)
    else:
        df_nouvelles_annees = pd.DataFrame()
    
    return {
        'nouveaux_indicateurs': df_nouveaux_indicateurs,
        'nouvelles_annees': df_nouvelles_annees,
        'donnees_a_updater': donnees_a_updater
    }


# ==========================
# INTERFACE
# ==========================

# Initialiser le session state
if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False
if 'df_staged' not in st.session_state:
    st.session_state.df_staged = None
if 'comparison' not in st.session_state:
    st.session_state.comparison = None
if 'indicateurs_titres' not in st.session_state:
    st.session_state.indicateurs_titres = {}
if 'confirmation_prod' not in st.session_state:
    st.session_state.confirmation_prod = False

# Bouton pour lancer la comparaison
col_b1, col_b2, col_b3 = st.columns([2, 3, 2])
with col_b2:
    if st.button("ğŸ” Analyser les donnÃ©es Ã  livrer", type="primary", use_container_width=True):
        
        with st.spinner("Chargement des titres d'indicateurs..."):
            # Charger le mapping id -> titre
            st.session_state.indicateurs_titres = load_indicateurs_titres()
        
        with st.spinner("Chargement des donnÃ©es staging..."):
            # Chargement des donnÃ©es staging d'abord
            df_staged_raw = load_staged_data()
        
        if df_staged_raw.empty:
            st.warning("âš ï¸ Aucune donnÃ©e dans la table staging `indicateurs_valeurs_olap`")
            st.session_state.analysis_done = False
            st.stop()
        
        with st.spinner("Transformation des donnÃ©es staging pour la prod (mapping ID)..."):
            # Transformation des IDs preprod â†’ prod
            st.session_state.df_staged = transform_staged_for_prod(df_staged_raw)
        
        if st.session_state.df_staged.empty:
            st.error("âŒ Aucune donnÃ©e aprÃ¨s transformation (vÃ©rifiez les mappings)")
            st.session_state.analysis_done = False
            st.stop()
        
        with st.spinner("Chargement des donnÃ©es production (filtrÃ©)..."):
            # Chargement des donnÃ©es production avec filtre sur les clÃ©s primaires du staging
            df_prod = load_prod_data(st.session_state.df_staged)
        
        with st.spinner("Comparaison en cours..."):
            st.session_state.comparison = compare_data(st.session_state.df_staged, df_prod)
        
        st.session_state.analysis_done = True
        st.session_state.confirmation_prod = False

# Fonction helper pour formater l'affichage des indicateurs
def format_indicateur(indic_id):
    """Retourne 'Titre' ou juste 'ID' si pas de titre."""
    titre = st.session_state.indicateurs_titres.get(indic_id, None)
    if titre:
        return f"{titre}"
    return str(indic_id)

# Afficher les rÃ©sultats si l'analyse a Ã©tÃ© faite
if st.session_state.analysis_done:
    df_staged = st.session_state.df_staged
    comparison = st.session_state.comparison
    
    # Statistiques globales
    st.markdown("---")
    st.markdown("## ğŸ“Š Statistiques globales")
    
    nb_indicateurs_staged = df_staged['indicateur_id'].nunique()
    nb_lignes_staged = len(df_staged)
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ğŸ“¦ Indicateurs staged", nb_indicateurs_staged)
    with col2:
        st.metric("ğŸ“ Lignes totales staged", f"{nb_lignes_staged:,}")
    
    # Liste des indicateurs staged
    with st.expander("ğŸ“‹ Liste des indicateurs staged"):
        indicateurs_info = df_staged.groupby('indicateur_id').agg({
            'collectivite_id': 'nunique',
            'date_valeur': ['min', 'max', 'count']
        }).reset_index()
        indicateurs_info.columns = ['Indicateur ID', 'Nb collectivitÃ©s', 'Date min', 'Date max', 'Nb lignes']
        
        # Ajouter le titre de l'indicateur
        indicateurs_info['Indicateur'] = indicateurs_info['Indicateur ID'].apply(format_indicateur)
        
        # RÃ©organiser les colonnes pour avoir le titre en premier
        indicateurs_info = indicateurs_info[['Indicateur', 'Nb collectivitÃ©s', 'Date min', 'Date max', 'Nb lignes']]
        
        st.dataframe(indicateurs_info, use_container_width=True)
    
    # Comparaison
    st.markdown("---")
    st.markdown("## ğŸ”„ Comparaison avec production")
    
    # Affichage des rÃ©sultats
    nb_nouveaux = len(comparison['nouveaux_indicateurs'])
    nb_nouvelles_annees = len(comparison['nouvelles_annees'])
    
    # Compter le nombre total de lignes Ã  updater
    nb_updates = sum(stats['nb_lignes'] for stats in comparison['donnees_a_updater'].values())
    nb_indicateurs_updates = len(comparison['donnees_a_updater'])
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ğŸ†• Nouveaux indicateurs", nb_nouveaux)
    with col2:
        st.metric("ğŸ“… Nouvelles annÃ©es", nb_nouvelles_annees)
    with col3:
        st.metric("ğŸ”„ DonnÃ©es Ã  updater", f"{nb_updates} ({nb_indicateurs_updates} indic.)")
    
    # --- NOUVEAUX INDICATEURS ---
    if nb_nouveaux > 0:
        st.markdown("---")
        st.markdown("### ğŸ†• Nouveaux indicateurs Ã  importer")
        st.info(f"Ces {nb_nouveaux} lignes correspondent Ã  des indicateurs qui n'existent pas encore en production.")
        
        # Grouper par indicateur
        indicateurs_nouveaux = comparison['nouveaux_indicateurs']['indicateur_id'].unique()
        
        for indic_id in indicateurs_nouveaux:
            df_indic = comparison['nouveaux_indicateurs'][
                comparison['nouveaux_indicateurs']['indicateur_id'] == indic_id
            ]
            
            nb_lignes = len(df_indic)
            nb_collectivites = df_indic['collectivite_id'].nunique()
            annees = sorted(df_indic['date_valeur'].dt.year.unique())
            
            with st.expander(
                f"ğŸ“Œ {format_indicateur(indic_id)} - {nb_lignes} lignes, {nb_collectivites} collectivitÃ©s"
            ):
                st.markdown(f"**AnnÃ©es :** {', '.join(map(str, annees))}")
                st.dataframe(df_indic, use_container_width=True, height=300)
    
    # --- NOUVELLES ANNÃ‰ES ---
    if nb_nouvelles_annees > 0:
        st.markdown("---")
        st.markdown("### ğŸ“… Nouvelles annÃ©es Ã  importer")
        st.info(f"Ces {nb_nouvelles_annees} lignes correspondent Ã  de nouvelles annÃ©es pour des indicateurs existants.")
        
        # Grouper par indicateur
        indicateurs_nouvelles_annees = comparison['nouvelles_annees']['indicateur_id'].unique()
        
        for indic_id in indicateurs_nouvelles_annees:
            df_indic = comparison['nouvelles_annees'][
                comparison['nouvelles_annees']['indicateur_id'] == indic_id
            ]
            
            nb_lignes = len(df_indic)
            nb_collectivites = df_indic['collectivite_id'].nunique()
            annees = sorted(df_indic['date_valeur'].dt.year.unique())
            
            with st.expander(
                f"ğŸ“Œ {format_indicateur(indic_id)} - {nb_lignes} nouvelles lignes, {nb_collectivites} collectivitÃ©s"
            ):
                st.markdown(f"**Nouvelles annÃ©es :** {', '.join(map(str, annees))}")
                st.dataframe(df_indic, use_container_width=True, height=300)
    
    # --- DONNÃ‰ES Ã€ UPDATER ---
    if nb_updates > 0:
        st.markdown("---")
        st.markdown("### ğŸ”„ DonnÃ©es Ã  mettre Ã  jour")
        st.warning(f"Ces {nb_updates} lignes (sur {nb_indicateurs_updates} indicateurs) ont des rÃ©sultats diffÃ©rents.")
        
        # Afficher par indicateur
        for indic_id, stats in comparison['donnees_a_updater'].items():
            with st.expander(f"ğŸ“Œ {format_indicateur(indic_id)} - {stats['nb_lignes']} ligne(s)", expanded=False):
                
                # Statistiques d'Ã©cart
                if stats.get('ecart_moyen_pct') is not None:
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.metric(
                            "ğŸ“Š Ã‰cart moyen", 
                            f"{stats['ecart_moyen_pct']:.0f}%",
                            help="Ã‰cart moyen en % pour cet indicateur"
                        )
                    
                    with col2:
                        st.metric(
                            "ğŸ“ˆ Ã‰cart maximum", 
                            f"{stats['ecart_max_pct']:.0f}%",
                            help="Ã‰cart maximum observÃ© pour cet indicateur"
                        )
                    
                    # DÃ©tail de l'Ã©cart maximum
                    st.markdown("**ğŸ” Ã‰cart maximum :**")
                    col_info1, col_info2 = st.columns(2)
                    with col_info1:
                        st.markdown(f"- **CollectivitÃ© ID:** {stats['collectivite_id_max']}")
                        st.markdown(f"- **Date:** {stats['date_valeur_max']}")
                    with col_info2:
                        st.markdown(f"- **Valeur production:** {stats['resultat_prod_max']}")
                        st.markdown(f"- **Valeur staging:** {stats['resultat_staged_max']}")
                else:
                    st.info("â„¹ï¸ " + stats.get('message', 'Aucun Ã©cart calculable'))
                
                # Afficher le dataframe complet
                st.markdown("---")
                st.markdown("**ğŸ“‹ DÃ©tails complets :**")
                st.dataframe(stats['dataframe'], use_container_width=True, height=300)
    
    # Message de synthÃ¨se
    if nb_nouveaux == 0 and nb_nouvelles_annees == 0 and nb_updates == 0:
        st.success("âœ… Aucune diffÃ©rence dÃ©tectÃ©e ! Les donnÃ©es staging sont identiques Ã  la production.")

# Bouton de livraison - en dehors du bloc d'analyse pour rester visible
if st.session_state.analysis_done and st.session_state.df_staged is not None:
    st.markdown("---")
    st.markdown("## ğŸš€ Livraison en PRODUCTION")
    
    # Calculer le nombre de lignes Ã  envoyer
    comparison = st.session_state.comparison
    nb_to_send = len(comparison['nouveaux_indicateurs']) + len(comparison['nouvelles_annees'])
    nb_to_send += sum(stats.get('nb_lignes', 0) for stats in comparison['donnees_a_updater'].values())
    
    if nb_to_send == 0:
        st.info("âœ… Aucune donnÃ©e Ã  livrer : tout est dÃ©jÃ  Ã  jour en production !")
    else:
        st.error(f"ğŸš¨ **ATTENTION : Cette action va envoyer {nb_to_send:,} lignes en PRODUCTION via API.**")
        
        # PrÃ©parer le DataFrame Ã  envoyer pour le tÃ©lÃ©chargement
        dfs_to_send = []
        if len(comparison['nouveaux_indicateurs']) > 0:
            dfs_to_send.append(comparison['nouveaux_indicateurs'])
        if len(comparison['nouvelles_annees']) > 0:
            dfs_to_send.append(comparison['nouvelles_annees'])
        for indic_id, stats in comparison['donnees_a_updater'].items():
            if 'dataframe' in stats:
                df_update = stats['dataframe'].copy()
                df_update = df_update[['collectivite_id', 'indicateur_id', 'metadonnee_id', 'date_valeur', 'resultat_staged']]
                df_update = df_update.rename(columns={'resultat_staged': 'resultat'})
                dfs_to_send.append(df_update)

        # DÃ©tail de ce qui sera envoyÃ©
        message_lines = []
        if len(comparison['nouveaux_indicateurs']) > 0:
            message_lines.append(f"ğŸ†• {len(comparison['nouveaux_indicateurs']):,} lignes de nouveaux indicateurs")
        if len(comparison['nouvelles_annees']) > 0:
            message_lines.append(f"ğŸ“… {len(comparison['nouvelles_annees']):,} lignes de nouvelles annÃ©es")
        nb_updates_lines = sum(stats.get('nb_lignes', 0) for stats in comparison['donnees_a_updater'].values())
        if nb_updates_lines > 0:
            message_lines.append(f"ğŸ”„ {nb_updates_lines:,} lignes Ã  mettre Ã  jour")
        st.info(" - ".join(message_lines))
        
        if dfs_to_send:
            df_to_download = pd.concat(dfs_to_send, ignore_index=True)
            csv_data = df_to_download.to_csv(index=False).encode('utf-8')
            
            st.download_button(
                label="ğŸ“¥ TÃ©lÃ©charger les donnÃ©es Ã  envoyer (CSV)",
                data=csv_data,
                file_name=f"donnees_a_livrer_prod_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )

    col_btn1, col_btn2, col_btn3 = st.columns([2, 3, 2])
    with col_btn2:
        if st.button("ğŸš¢ğŸš¨ Livrer en PRODUCTION", type="primary", use_container_width=True, disabled=(nb_to_send == 0)):
            st.session_state.confirmation_prod = True
    
    # Message de confirmation aprÃ¨s le clic sur le bouton
    if st.session_state.confirmation_prod:
        st.markdown("---")
        st.error("### âš ï¸ CONFIRMATION REQUISE")
        st.warning("**ÃŠtes-vous sÃ»r de vouloir livrer en production ?**")
        st.markdown("""
        Cette action est **irrÃ©versible** et va modifier les donnÃ©es en **PRODUCTION**.
        
        âœ… Assurez-vous d'avoir :
        - VÃ©rifiÃ© les donnÃ©es Ã  livrer
        - TestÃ© en prÃ©-production
        - L'autorisation nÃ©cessaire
        """)
        
        col_conf1, col_conf2, col_conf3 = st.columns([1, 1, 1])
        
        with col_conf1:
            if st.button("âŒ Annuler", use_container_width=True):
                st.session_state.confirmation_prod = False
                st.rerun()
        
        with col_conf3:
            if st.button("âœ… OUI, LIVRER EN PROD", type="primary", use_container_width=True):
                # Container pour la progression
                progress_container = st.container()
                
                result = livrer_en_prod(st.session_state.comparison, st.session_state.df_staged, progress_container)
                
                st.markdown("---")
                
                if result['success']:
                    st.success(f"âœ… {result['message']}")
                    
                    # Afficher les statistiques avec ou sans le filtrage
                    if result.get('nb_filtered', 0) > 0:
                        col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4)
                        with col_stat1:
                            st.metric("ğŸ“Š Total de lignes", f"{result['nb_total']:,}")
                        with col_stat2:
                            st.metric("ğŸ“¤ Lignes insÃ©rÃ©es", f"{result['nb_inserted']:,}")
                        with col_stat3:
                            st.metric("ğŸš« Lignes filtrÃ©es", f"{result['nb_filtered']:,}")
                        with col_stat4:
                            st.metric("ğŸ“¦ Batches envoyÃ©s", result.get('nb_batches', 0))
                    else:
                        col_stat1, col_stat2, col_stat3 = st.columns(3)
                        with col_stat1:
                            st.metric("ğŸ“Š Total de lignes", f"{result['nb_total']:,}")
                        with col_stat2:
                            st.metric("ğŸ“¤ Lignes insÃ©rÃ©es", f"{result['nb_inserted']:,}")
                        with col_stat3:
                            st.metric("ğŸ“¦ Batches envoyÃ©s", result.get('nb_batches', 0))
                    
                    st.info("ğŸ’¡ Vous pouvez relancer l'analyse pour vÃ©rifier que les donnÃ©es ont bien Ã©tÃ© livrÃ©es.")
                else:
                    st.error(f"âŒ {result['message']}")
                    
                    if result.get('failed_batches', 0) > 0:
                        st.warning(f"âš ï¸ {result['failed_batches']} batch(s) ont Ã©chouÃ© sur {result.get('nb_batches', 0)} total")
                        st.info(f"ğŸ’¡ {result['nb_inserted']:,} lignes ont quand mÃªme Ã©tÃ© insÃ©rÃ©es avec succÃ¨s")
                
                # RÃ©initialiser la confirmation
                st.session_state.confirmation_prod = False

